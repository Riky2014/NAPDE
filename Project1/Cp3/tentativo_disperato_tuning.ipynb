{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riky2014/NAPDE/blob/main/Project1/Cp3/tentativo_disperato_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z-Qdx7crGeGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ecf95f-967c-41d2-9c31-6490ad2583e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cvxpy as cvx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "!pip -q install pyDOE\n",
        "from pyDOE import lhs\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Alepescinaa/ScientificTools\n",
        "%cd ScientificTools/Project1/Cp3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNdT-d57GhPu",
        "outputId": "dfce7f26-30db-4adb-c061-10b1900c74d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ScientificTools'...\n",
            "remote: Enumerating objects: 672, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 672 (delta 34), reused 1 (delta 1), pack-reused 601\u001b[K\n",
            "Receiving objects: 100% (672/672), 140.57 MiB | 25.80 MiB/s, done.\n",
            "Resolving deltas: 100% (232/232), done.\n",
            "/content/ScientificTools/Project1/Cp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jCk9Ral8GeGO"
      },
      "outputs": [],
      "source": [
        "CP3data = np.load(\"CP3data.npz\")\n",
        "CP3data = CP3data['arr_0']\n",
        "\n",
        "CP3estimate = np.load(\"CP3field.npz\")\n",
        "CP3estimate = CP3estimate['arr_0']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mesh_size = 151**2\n",
        "num_samples = 100\n",
        "A = np.zeros((mesh_size, num_samples))\n",
        "\n",
        "for i in range(num_samples):\n",
        "  A[:, i] = CP3estimate[i][0].flatten()\n",
        "\n",
        "[U,s,V] = np.linalg.svd(A, full_matrices=False)\n",
        "\n",
        "k = 10\n",
        "p = 100\n",
        "perm = np.random.choice(mesh_size, size=p, replace=False)\n",
        "\n",
        "C = np.zeros((p,mesh_size))\n",
        "for i in range(p):\n",
        "    C[i,perm[i]] = 1.0\n",
        "\n",
        "Theta = C @ U\n",
        "coeff_mat = np.zeros((k, num_samples))\n",
        "\n",
        "for i in range (num_samples):\n",
        "  u = A[:,i]\n",
        "  y = C @ u\n",
        "  s_c = cvx.Variable(U.shape[1])\n",
        "  constraints = [Theta @ s_c == y]\n",
        "  obj = cvx.Minimize(cvx.norm(s_c, 1))\n",
        "  prob = cvx.Problem(obj, constraints)\n",
        "  prob.solve()\n",
        "  coeff_mat[:, i] = s_c.value[:k]\n",
        "\n",
        "c1_min, c1_max = np.min(coeff_mat[0, :]), np.max(coeff_mat[0, :])\n",
        "c2_min, c2_max = np.min(coeff_mat[1, :]), np.max(coeff_mat[1, :])\n",
        "c3_min, c3_max = np.min(coeff_mat[2, :]), np.max(coeff_mat[2, :])\n",
        "c4_min, c4_max = np.min(coeff_mat[3, :]), np.max(coeff_mat[3, :])\n",
        "c5_min, c5_max = np.min(coeff_mat[4, :]), np.max(coeff_mat[4, :])\n",
        "c6_min, c6_max = np.min(coeff_mat[5, :]), np.max(coeff_mat[5, :])\n",
        "c7_min, c7_max = np.min(coeff_mat[6, :]), np.max(coeff_mat[6, :])\n",
        "c8_min, c8_max = np.min(coeff_mat[7, :]), np.max(coeff_mat[7, :])\n",
        "c9_min, c9_max = np.min(coeff_mat[8, :]), np.max(coeff_mat[8, :])\n",
        "c10_min, c10_max = np.min(coeff_mat[9, :]), np.max(coeff_mat[9, :])\n",
        "\n",
        "delta_c1 = c1_max - c1_min\n",
        "delta_c2 = c2_max - c2_min\n",
        "delta_c3 = c3_max - c3_min\n",
        "delta_c4 = c4_max - c4_min\n",
        "delta_c5 = c5_max - c5_min\n",
        "delta_c6 = c6_max - c6_min\n",
        "delta_c7 = c7_max - c7_min\n",
        "delta_c8 = c8_max - c8_min\n",
        "delta_c9 = c9_max - c9_min\n",
        "delta_c10 = c10_max - c10_min\n",
        "\n",
        "print(f\"c1 in ({c1_min}, {c1_max})\")\n",
        "print(f\"c2 in ({c2_min}, {c2_max})\")\n",
        "print(f\"c3 in ({c3_min}, {c3_max})\")\n",
        "print(f\"c4 in ({c4_min}, {c4_max})\")\n",
        "print(f\"c5 in ({c5_min}, {c5_max})\")\n",
        "print(f\"c6 in ({c6_min}, {c6_max})\")\n",
        "print(f\"c7 in ({c7_min}, {c7_max})\")\n",
        "print(f\"c8 in ({c8_min}, {c8_max})\")\n",
        "print(f\"c9 in ({c9_min}, {c9_max})\")\n",
        "print(f\"c10 in ({c10_min}, {c10_max})\")\n",
        "\n",
        "basis = U[:, :k]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnPdd3cvwaCv",
        "outputId": "d0a07170-044b-4198-c971-bc0732475f6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c1 in (-14901.762094931402, -12144.026767259482)\n",
            "c2 in (-1643.3543282356468, 1582.2151985821024)\n",
            "c3 in (-1666.6222565459314, 1707.616360833575)\n",
            "c4 in (-776.3212422610503, 1280.585831484112)\n",
            "c5 in (-1093.9752515232494, 1047.6008504668707)\n",
            "c6 in (-717.0424992233817, 537.6185968969928)\n",
            "c7 in (-469.9943812071792, 438.8180282175097)\n",
            "c8 in (-328.7863116122087, 528.7552626498631)\n",
            "c9 in (-257.8055527194194, 317.60595986928547)\n",
            "c10 in (-216.0399309556303, 357.6237000279386)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collocation points\n",
        "Ncl = 151**2\n",
        "Xcl = lhs(2,Ncl)\n",
        "xcl = tf.expand_dims(tf.cast(-1.5+(3.0)*Xcl[:,0],dtype=tf.float64),axis=-1)\n",
        "ycl = tf.expand_dims(tf.cast(-1.5+(3.0)*Xcl[:,1],dtype=tf.float64),axis=-1)\n",
        "X_coll = tf.concat([xcl,ycl],1)"
      ],
      "metadata": {
        "id": "gE6IjTv_BqBg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cvDwkpE9BDRH"
      },
      "outputs": [],
      "source": [
        "def penalty(param, lower_bound, upper_bound):\n",
        "    return tf.reduce_sum(tf.square(tf.maximum(param - upper_bound, 0)) +\n",
        "                         tf.square(tf.maximum(lower_bound - param, 0)))\n",
        "\n",
        "# PINN loss function\n",
        "def loss(xcl,ycl,xmeas,ymeas,umeas,coeff,PINN):\n",
        "    input_data=tf.concat([xmeas,ymeas],1)\n",
        "    umeas_pred = PINN(input_data)\n",
        "    r_pred   = r_PINN(xcl,ycl,coeff_1,coeff_2,coeff_3,coeff_other)\n",
        "\n",
        "    # loss components\n",
        "    mse_meas  = tf.reduce_mean(tf.pow(umeas-umeas_pred,2))\n",
        "    mse_r  = tf.reduce_mean(tf.abs(r_pred))\n",
        "\n",
        "    # bc\n",
        "    y0 = tf.constant([-0.517409965],dtype=tf.float64)\n",
        "    mse_bc= tf.pow( PINN( tf.transpose( tf.stack( [tf.constant([1.5],dtype=tf.float64), y0] ) ) ) ,2)\n",
        "\n",
        "    #penalty\n",
        "    mse_penalty = penalty(coeff_other[0],0,1)+penalty(coeff_other[1],0,1)+penalty(coeff_other[2],0,1)+penalty(coeff_other[3],0,1)+penalty(coeff_other[4],0,1)+penalty(coeff_other[5],0,1)+penalty(coeff_other[6],0,1)+penalty(coeff_1,0,1)+penalty(coeff_2,0,1)+penalty(coeff_3,0,1)\n",
        "\n",
        "    return mse_meas + mse_r + mse_bc + mse_penalty\n",
        "\n",
        "def loss2(xcl,ycl,xmeas,ymeas,umeas,PINN):\n",
        "    input_data=tf.concat([xmeas,ymeas],1)\n",
        "    umeas_pred = PINN(input_data)\n",
        "    r_pred   = r_PINN2(xcl,ycl,PINN)\n",
        "\n",
        "    # loss components\n",
        "    mse_meas  = tf.reduce_mean(tf.pow(umeas-umeas_pred,2))\n",
        "    mse_r  = tf.reduce_mean(tf.abs(r_pred))\n",
        "\n",
        "    # bc\n",
        "    y0 = tf.constant([-0.517409965],dtype=tf.float64)\n",
        "    mse_bc= tf.pow(PINN( tf.transpose( tf.stack( [tf.constant([1.5],dtype=tf.float64), y0] ))  ) ,2)\n",
        "\n",
        "    return mse_meas + mse_r + mse_bc\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def r_PINN(x,y,coeff,PINN):\n",
        "    input_data=tf.concat([x,y],1)\n",
        "    u = PINN(input_data)\n",
        "    u_x = tf.gradients(u,x)[0]\n",
        "    u_y = tf.gradients(u,y)[0]\n",
        "    u_grad = tf.transpose(tf.concat([u_x, u_y], axis=1))\n",
        "\n",
        "    pi = tf.constant(np.pi,dtype=tf.float64)\n",
        "    theta_fiber = tf.constant([0.13757033666666668] ,dtype=tf.float64)\n",
        "    a_ratio = tf.constant([5.896298503333333], dtype=tf.float64)\n",
        "    theta0 = pi/2 - theta_fiber\n",
        "\n",
        "    a = tf.stack([tf.cos(theta0), tf.sin(theta0)])\n",
        "    b = tf.stack([tf.cos(theta0-pi/2), tf.sin(theta0-pi/2)])\n",
        "\n",
        "    D_00 = 1 / a_ratio * a[0]**2 + b[0]**2\n",
        "    D_01 = 1 / a_ratio * a[0] * a[1] + b[0] * b[1]\n",
        "    D_10 = 1 / a_ratio * a[0] * a[1] + b[0] * b[1]\n",
        "    D_11 = 1 / a_ratio * a[1]**2 + b[1]**2\n",
        "\n",
        "    c1 = c1_min + delta_c1 * coeff_1\n",
        "    c2 = c2_min + delta_c2 * coeff_2\n",
        "    c3 = c3_min + delta_c3 * coeff_3\n",
        "    c4 = c4_min + delta_c4 * coeff_other[0]\n",
        "    c5 = c5_min + delta_c5 * coeff_other[1]\n",
        "    c6 = c6_min + delta_c6 * coeff_other[2]\n",
        "    c7 = c7_min + delta_c7 * coeff_other[3]\n",
        "    c8 = c8_min + delta_c8 * coeff_other[4]\n",
        "    c9 = c9_min + delta_c9 * coeff_other[5]\n",
        "    c10 = c10_min + delta_c10 * coeff_other[6]\n",
        "\n",
        "    coeff_true = tf.expand_dims(tf.concat([c1[0],c2[0],c3[0],c4,c5,c6,c7,c8,c9,c10], 0), 1)\n",
        "\n",
        "    return   (((u_x * D_00 * u_x + u_x * D_01 * u_y + u_y * D_10 * u_x + u_y * D_11 * u_y)))  - (1/(basis@coeff_true))**2\n",
        "\n",
        "@tf.function\n",
        "def r_PINN2(x,y,PINN):\n",
        "    input_data=tf.concat([x,y],1)\n",
        "    u = PINN(input_data)\n",
        "    u_x = tf.gradients(u,x)[0]\n",
        "    u_y = tf.gradients(u,y)[0]\n",
        "    u_grad = tf.transpose(tf.concat([u_x, u_y], axis=1))\n",
        "\n",
        "    pi = tf.constant(np.pi,dtype=tf.float64)\n",
        "    theta_fiber = tf.constant([0.13757033666666668] ,dtype=tf.float64)\n",
        "    a_ratio = tf.constant([5.896298503333333], dtype=tf.float64)\n",
        "    theta0 = pi/2 - theta_fiber\n",
        "\n",
        "    a = tf.stack([tf.cos(theta0), tf.sin(theta0)])\n",
        "    b = tf.stack([tf.cos(theta0-pi/2), tf.sin(theta0-pi/2)])\n",
        "\n",
        "    D_00 = 1 / a_ratio * a[0]**2 + b[0]**2\n",
        "    D_01 = 1 / a_ratio * a[0] * a[1] + b[0] * b[1]\n",
        "    D_10 = 1 / a_ratio * a[0] * a[1] + b[0] * b[1]\n",
        "    D_11 = 1 / a_ratio * a[1]**2 + b[1]**2\n",
        "\n",
        "    return  (((u_x * D_00 * u_x + u_x * D_01 * u_y + u_y * D_10 * u_x + u_y * D_11 * u_y)))  - (1/100)**2\n",
        "\n",
        "\n",
        "# neural network weight gradients\n",
        "@tf.function\n",
        "def grad(model,xcl,ycl,xmeas,ymeas,umeas,coeff):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        loss_value = loss(xcl,ycl,xmeas,ymeas,umeas,coeff,model)\n",
        "        grads = tape.gradient(loss_value,model.trainable_variables)\n",
        "        grads_weight= tape.gradient(loss_value,coeff)\n",
        "    return loss_value, grads, grads_weight\n",
        "    # neural network weight gradients\n",
        "\n",
        "@tf.function\n",
        "def grad2(model,xcl,ycl,xmeas,ymeas,umeas):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        loss_value2 = loss2(xcl,ycl,xmeas,ymeas,umeas,model)\n",
        "        grads2 = tape.gradient(loss_value2,model.trainable_variables)\n",
        "    return loss_value2, grads2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "def build_pinn(regularization_strength):\n",
        "    PINN = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(32, activation='relu', input_shape=(2,),\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64),\n",
        "        tf.keras.layers.Dense(64, activation='relu',\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64),\n",
        "        tf.keras.layers.Dense(128, activation='relu',\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64),\n",
        "        tf.keras.layers.Dense(64, activation='relu',\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64),\n",
        "        tf.keras.layers.Dense(32, activation='relu',\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64),\n",
        "        tf.keras.layers.Dense(1, activation=None,\n",
        "                              kernel_initializer=\"glorot_uniform\",\n",
        "                              kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                              dtype=tf.float64)\n",
        "    ])\n",
        "    return PINN\n",
        "\n"
      ],
      "metadata": {
        "id": "tM51pcFCBvdj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkpoint2_solution( x, y , t, regularization_strength):\n",
        "\n",
        "  PINN = build_pinn(regularization_strength)\n",
        "\n",
        "  xmeas_train, xmeas_val, ymeas_train, ymeas_val, tmeas_train, tmeas_val = train_test_split(x, y, t, test_size=0.05)\n",
        "  xmeas_train = tf.constant(xmeas_train.reshape(19, 1), dtype=tf.float64)\n",
        "  ymeas_train = tf.constant(ymeas_train.reshape(19, 1), dtype=tf.float64)\n",
        "  tmeas_train = tf.constant(tmeas_train.reshape(19, 1), dtype=tf.float64)\n",
        "  xmeas_val = tf.constant(xmeas_val.reshape(1, 1), dtype=tf.float64)\n",
        "  ymeas_val = tf.constant(ymeas_val.reshape(1, 1), dtype=tf.float64)\n",
        "  tmeas_val = tf.constant(tmeas_val.reshape(1, 1), dtype=tf.float64)\n",
        "\n",
        "  X, Y = np.meshgrid(np.linspace(-1.5,1.5,151), np.linspace(-1.5,1.5,151))\n",
        "\n",
        "  coeff_1 = tf.Variable([[0.5]], trainable=True, dtype=tf.float64)\n",
        "  coeff_2 = tf.Variable([[0.5]], trainable=True, dtype=tf.float64)\n",
        "  coeff_3 = tf.Variable([[0.5]], trainable=True, dtype=tf.float64)\n",
        "  coeff_other = tf.Variable([[0.5],[0.5],[0.5],[0.5],[0.5],[0.5],[0.5]], trainable=True, dtype=tf.float64)\n",
        "\n",
        "  tf_optimizer_PINN = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99)\n",
        "  tf_optimizer_PINN.build(PINN.trainable_variables)\n",
        "  tf_optimizer_c1 = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99)\n",
        "  tf_optimizer_c1.build([coeff_1])\n",
        "  tf_optimizer_c2 = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99)\n",
        "  tf_optimizer_c2.build([coeff_2])\n",
        "  tf_optimizer_c3 = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99)\n",
        "  tf_optimizer_c3.build([coeff_3])\n",
        "  tf_optimizer_other = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.99)\n",
        "  tf_optimizer_other.build([coeff_other])\n",
        "\n",
        "\n",
        "  print()\n",
        "\n",
        "  for iter in range(100):\n",
        "\n",
        "    loss_value2,grads2 = grad2(PINN, xcl,ycl,xmeas_train, ymeas_train, tmeas_train)\n",
        "\n",
        "    tf_optimizer_PINN.apply_gradients(zip(grads2,PINN.trainable_variables))\n",
        "\n",
        "    loss_value_val2, _= grad2(PINN, xcl, ycl, xmeas_val, ymeas_val, tmeas_val)\n",
        "\n",
        "\n",
        "    if ((iter+1) % 100 == 0):\n",
        "      print('iter =  '+str(iter+1))\n",
        "      tf.print('loss =' , loss_value2)\n",
        "      tf.print('loss_val_param =' , loss_value_val2)\n",
        "\n",
        "      c1 = c1_min + delta_c1 * coeff_1\n",
        "      c2 = c2_min + delta_c2 * coeff_2\n",
        "      c3 = c3_min + delta_c3 * coeff_3\n",
        "      c4 = c4_min + delta_c4 * coeff_other[0]\n",
        "      c5 = c5_min + delta_c5 * coeff_other[1]\n",
        "      c6 = c6_min + delta_c6 * coeff_other[2]\n",
        "      c7 = c7_min + delta_c7 * coeff_other[3]\n",
        "      c8 = c8_min + delta_c8 * coeff_other[4]\n",
        "      c9 = c9_min + delta_c9 * coeff_other[5]\n",
        "      c10 = c10_min + delta_c10 * coeff_other[6]\n",
        "\n",
        "      coeff_true = tf.expand_dims(tf.concat([c1[0],c2[0],c3[0],c4,c5,c6,c7,c8,c9,c10], 0), 1)\n",
        "      print(coeff_true.numpy())\n",
        "      print()\n",
        "\n",
        "\n",
        "  for iter in range(100):\n",
        "\n",
        "    loss_value,grads,grads_coeff = grad(PINN,xcl,ycl,xmeas_train, ymeas_train, tmeas_train,coeff)\n",
        "\n",
        "    tf_optimizer_PINN.apply_gradients(zip(grads ,PINN.trainable_variables))\n",
        "    tf_optimizer_weights.apply_gradients(zip([grads_coeff], [coeff]))\n",
        "\n",
        "    loss_value_val, _, _ = grad(PINN, xcl, ycl, xmeas_val, ymeas_val, tmeas_val,coeff)\n",
        "\n",
        "\n",
        "    if ((iter+1) % 100 == 0):\n",
        "      print('iter =  '+str(iter+1))\n",
        "      tf.print('loss =' , loss_value)\n",
        "      tf.print('loss_val_param =' , loss_value_val)\n",
        "\n",
        "      c1 = c1_min + delta_c1 * coeff_1\n",
        "      c2 = c2_min + delta_c2 * coeff_2\n",
        "      c3 = c3_min + delta_c3 * coeff_3\n",
        "      c4 = c4_min + delta_c4 * coeff_other[0]\n",
        "      c5 = c5_min + delta_c5 * coeff_other[1]\n",
        "      c6 = c6_min + delta_c6 * coeff_other[2]\n",
        "      c7 = c7_min + delta_c7 * coeff_other[3]\n",
        "      c8 = c8_min + delta_c8 * coeff_other[4]\n",
        "      c9 = c9_min + delta_c9 * coeff_other[5]\n",
        "      c10 = c10_min + delta_c10 * coeff_other[6]\n",
        "\n",
        "      coeff_true = tf.expand_dims(tf.concat([c1[0],c2[0],c3[0],c4,c5,c6,c7,c8,c9,c10], 0), 1)\n",
        "      print(coeff_true.numpy())\n",
        "      print()\n",
        "\n",
        "  return regularization_strength, loss_value_val"
      ],
      "metadata": {
        "id": "uST9tU64B9fa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_random_search = 10\n",
        "regularization_strengths = np.logspace(-6, 0, num=num_random_search)\n",
        "regularization_strengths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f55RDRCWIWVA",
        "outputId": "2d50dfa9-31a7-4084-ebcb-8991b1d6325d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.00000000e-06, 4.64158883e-06, 2.15443469e-05, 1.00000000e-04,\n",
              "       4.64158883e-04, 2.15443469e-03, 1.00000000e-02, 4.64158883e-02,\n",
              "       2.15443469e-01, 1.00000000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind_disp = 64\n",
        "x_meas = CP3data[ind_disp][0]\n",
        "y_meas = CP3data[ind_disp][1]\n",
        "t_meas = CP3data[ind_disp][2]\n",
        "speed_field = CP3estimate[ind_disp][0]\n",
        "\n",
        "num_random_search = 10\n",
        "regularization_strengths = np.logspace(-6, 0, num=num_random_search)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_regularization_strength = None\n",
        "\n",
        "\n",
        "for regularization_strength in regularization_strengths:\n",
        "  regularization_strength, loss_value_val= checkpoint2_solution(x_meas, y_meas, t_meas,regularization_strength)\n",
        "  if  loss_value_val < best_val_loss:\n",
        "      best_val_loss =  loss_value_val\n",
        "      best_regularization_strength = regularization_strength\n",
        "  print(f'Best Regularization strength: {best_regularization_strength}, Best Validation Loss: {best_val_loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2YBEKL3FxSI",
        "outputId": "186d390d-2963-41d3-c73d-4992f38deffa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f29581f3520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f29581f3520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter =  100\n",
            "loss = [[0.00037162011846411631]]\n",
            "loss_val_param = [[0.00059736754708933479]]\n",
            "[[-13522.8944311 ]\n",
            " [   -30.56956483]\n",
            " [    20.49705214]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817649]\n",
            " [    99.98447552]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00011093410504335435]]\n",
            "loss_val_param = [[0.00016792537889083063]]\n",
            "[[-1.48126712e+04]\n",
            " [ 6.99162361e+01]\n",
            " [-7.27507446e+01]\n",
            " [-7.48053691e+02]\n",
            " [-3.27987734e+00]\n",
            " [ 1.43119631e+02]\n",
            " [ 7.73242639e+01]\n",
            " [ 7.51417565e+01]\n",
            " [ 7.59406411e+01]\n",
            " [-2.11335994e+01]]\n",
            "\n",
            "Best Regularization strength: 1e-06, Best Validation Loss: [[0.00016793]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00090673905277941142]]\n",
            "loss_val_param = [[0.0018929132042861648]]\n",
            "[[-13522.8944311 ]\n",
            " [   -30.56956483]\n",
            " [    20.49705214]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817649]\n",
            " [    99.98447552]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.0005294514338008633]]\n",
            "loss_val_param = [[0.00052593128540795485]]\n",
            "[[-12817.38091023]\n",
            " [   106.70063116]\n",
            " [    85.71729726]\n",
            " [  -541.49807677]\n",
            " [   229.39841693]\n",
            " [  -115.23245637]\n",
            " [    71.71604789]\n",
            " [   151.44023252]\n",
            " [    67.28798316]\n",
            " [    60.8716504 ]]\n",
            "\n",
            "Best Regularization strength: 1e-06, Best Validation Loss: [[0.00016793]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00027327475909372446]]\n",
            "loss_val_param = [[0.00027100426212014268]]\n",
            "[[-13522.8944311 ]\n",
            " [   -30.56956483]\n",
            " [    20.49705214]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817649]\n",
            " [    99.98447552]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00012900464902789458]]\n",
            "loss_val_param = [[0.00011999591761195561]]\n",
            "[[-14841.86621194]\n",
            " [   142.41961936]\n",
            " [   -92.77504842]\n",
            " [  -676.36214564]\n",
            " [   114.52068585]\n",
            " [    99.79523786]\n",
            " [    64.06414468]\n",
            " [    89.95499519]\n",
            " [    33.02504148]\n",
            " [   -33.98433595]]\n",
            "\n",
            "Best Regularization strength: 2.154434690031882e-05, Best Validation Loss: [[0.00012]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.0010521958386527387]]\n",
            "loss_val_param = [[0.00084258761282262479]]\n",
            "[[-13522.8944311 ]\n",
            " [   -30.56956483]\n",
            " [    20.49705214]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817649]\n",
            " [    99.98447552]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00022359618787300267]]\n",
            "loss_val_param = [[0.00028414384443241564]]\n",
            "[[-1.45474503e+04]\n",
            " [ 5.11620286e+01]\n",
            " [-2.65407911e+01]\n",
            " [-6.70900079e+02]\n",
            " [ 1.96087670e+02]\n",
            " [ 7.92975012e+01]\n",
            " [ 6.68869127e+01]\n",
            " [ 6.26849167e+01]\n",
            " [ 9.46601126e+01]\n",
            " [-6.58117659e+00]]\n",
            "\n",
            "Best Regularization strength: 2.154434690031882e-05, Best Validation Loss: [[0.00012]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.0002701977291296274]]\n",
            "loss_val_param = [[0.00048425860223657609]]\n",
            "[[-13522.8944311 ]\n",
            " [   -30.56956483]\n",
            " [    20.49705214]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817649]\n",
            " [    99.98447552]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00022241928969820988]]\n",
            "loss_val_param = [[0.000369093918697336]]\n",
            "[[-14707.24536598]\n",
            " [    71.46530647]\n",
            " [  -129.35365068]\n",
            " [  -696.10184539]\n",
            " [   -22.61129184]\n",
            " [   103.21743388]\n",
            " [    55.60789089]\n",
            " [    71.14257805]\n",
            " [    95.25956215]\n",
            " [   -96.78491893]]\n",
            "\n",
            "Best Regularization strength: 2.154434690031882e-05, Best Validation Loss: [[0.00012]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00069833540584086]]\n",
            "loss_val_param = [[0.00086684979484432355]]\n",
            "[[-13522.8944311 ]\n",
            " [   -30.56956483]\n",
            " [    20.49705214]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817649]\n",
            " [    99.98447552]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00024260821862290505]]\n",
            "loss_val_param = [[0.00011463263552505579]]\n",
            "[[-1.40846008e+04]\n",
            " [-8.93982046e+01]\n",
            " [-8.27565346e+01]\n",
            " [-7.25501669e+02]\n",
            " [-7.63220132e+01]\n",
            " [ 1.51185585e+02]\n",
            " [ 4.72582413e+01]\n",
            " [ 3.77174821e+01]\n",
            " [ 7.79982479e+01]\n",
            " [ 8.71516426e+00]]\n",
            "\n",
            "Best Regularization strength: 0.002154434690031882, Best Validation Loss: [[0.00011463]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00071702977961028876]]\n",
            "loss_val_param = [[0.0005833513898858]]\n",
            "[[-13522.8944311 ]\n",
            " [   -30.56956483]\n",
            " [    20.49705214]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817649]\n",
            " [    99.98447552]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.000142291009260486]]\n",
            "loss_val_param = [[0.00016439909309204375]]\n",
            "[[-1.46611592e+04]\n",
            " [ 2.35570024e+00]\n",
            " [-8.38000307e+01]\n",
            " [-6.63892314e+02]\n",
            " [-1.69830134e+01]\n",
            " [ 1.33833246e+02]\n",
            " [ 8.78481010e+01]\n",
            " [ 1.05769889e+02]\n",
            " [ 8.36816472e+01]\n",
            " [-5.57221946e+01]]\n",
            "\n",
            "Best Regularization strength: 0.002154434690031882, Best Validation Loss: [[0.00011463]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00040258355760990408]]\n",
            "loss_val_param = [[0.00059569039801345342]]\n",
            "[[-13522.8944311 ]\n",
            " [   -30.56956483]\n",
            " [    20.49705214]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817649]\n",
            " [    99.98447552]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00019858781811191422]]\n",
            "loss_val_param = [[0.00015468245855540684]]\n",
            "[[-1.34143118e+04]\n",
            " [ 2.55135073e+00]\n",
            " [-5.06734859e+00]\n",
            " [-7.73249115e+02]\n",
            " [ 6.64437208e+01]\n",
            " [ 6.89354365e+01]\n",
            " [ 1.85227170e+01]\n",
            " [ 5.95671299e+01]\n",
            " [ 4.23116173e+01]\n",
            " [-4.75394318e+01]]\n",
            "\n",
            "Best Regularization strength: 0.002154434690031882, Best Validation Loss: [[0.00011463]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00027762745378929289]]\n",
            "loss_val_param = [[0.00097029655758300976]]\n",
            "[[-13522.8944311 ]\n",
            " [   -30.56956483]\n",
            " [    20.49705214]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817649]\n",
            " [    99.98447552]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00034788149554354096]]\n",
            "loss_val_param = [[0.00031715962001001822]]\n",
            "[[-1.39491938e+04]\n",
            " [-6.54174041e+01]\n",
            " [-7.05496356e+01]\n",
            " [-7.08319269e+02]\n",
            " [ 2.35925262e+01]\n",
            " [ 2.93341056e+01]\n",
            " [ 1.05338955e+02]\n",
            " [ 1.17681709e+02]\n",
            " [ 4.24235846e+01]\n",
            " [-1.11314200e+00]]\n",
            "\n",
            "Best Regularization strength: 0.002154434690031882, Best Validation Loss: [[0.00011463]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.00072417986840068907]]\n",
            "loss_val_param = [[0.0002780282212085925]]\n",
            "[[-13522.8944311 ]\n",
            " [   -30.56956483]\n",
            " [    20.49705214]\n",
            " [   252.13229461]\n",
            " [   -23.18720053]\n",
            " [   -89.71195116]\n",
            " [   -15.58817649]\n",
            " [    99.98447552]\n",
            " [    29.90020357]\n",
            " [    70.79188454]]\n",
            "\n",
            "iter =  100\n",
            "loss = [[0.0011529196093050275]]\n",
            "loss_val_param = [[0.001470900701884456]]\n",
            "[[-14856.0848052 ]\n",
            " [    96.73584764]\n",
            " [    39.38767401]\n",
            " [  -682.36563638]\n",
            " [   140.50822664]\n",
            " [    69.42078009]\n",
            " [   116.97254292]\n",
            " [    37.07452175]\n",
            " [    66.84005938]\n",
            " [   -25.51184859]]\n",
            "\n",
            "Best Regularization strength: 0.002154434690031882, Best Validation Loss: [[0.00011463]]\n"
          ]
        }
      ]
    }
  ]
}