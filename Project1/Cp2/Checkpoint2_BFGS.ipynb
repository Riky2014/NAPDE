{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riky2014/NAPDE/blob/main/Project1/Cp2/Checkpoint2_BFGS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uNmk-BAaFciZ"
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip -q install pyDOE\n",
        "from pyDOE import lhs  # for latin hypercube sampling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Alepescinaa/ScientificTools\n",
        "%cd ScientificTools/Project1/Cp2\n",
        "\n",
        "CP2data = np.load(\"CP2data.npz\")\n",
        "CP2data = CP2data['arr_0']"
      ],
      "metadata": {
        "id": "4mw9LKX9KWRm",
        "outputId": "1a81b56c-afb8-4305-afdf-df40c2764703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ScientificTools' already exists and is not an empty directory.\n",
            "/content/ScientificTools/Project1/Cp2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "    Optimize a keras model using scipy.optimize\n",
        "    This block of code is taken and adapted from https://github.com/pedro-r-marques/keras-opt/tree/master\n",
        "    See the repository for all the information.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K  # pylint: disable=import-error\n",
        "\n",
        "from tensorflow.python.keras.engine import data_adapter\n",
        "\n",
        "\n",
        "class ScipyOptimizer():\n",
        "    \"\"\" Implements a training function that uses scipy optimize in order\n",
        "        to determine the weights for the model.\n",
        "\n",
        "        The minimize function expects to be able to attempt multiple solutions\n",
        "        over the model. It calls a function which collects all gradients for\n",
        "        all steps and then returns the gradient information to the optimizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, method='BFGS', verbose=1, maxiter=1):\n",
        "        self.model = model\n",
        "        self.method = method\n",
        "        self.verbose = verbose\n",
        "        self.maxiter = maxiter\n",
        "        if model.run_eagerly:\n",
        "            self.func = model.__call__\n",
        "        else:\n",
        "            self.func = tf.function(\n",
        "                model.__call__, experimental_relax_shapes=True)\n",
        "\n",
        "    def _update_weights(self, x):\n",
        "        x_offset = 0\n",
        "        for var in self.model.trainable_variables:\n",
        "            shape = var.get_shape()\n",
        "            w_size = np.prod(shape)\n",
        "            value = np.array(x[x_offset:x_offset+w_size]).reshape(shape)\n",
        "            K.set_value(var, value)\n",
        "            x_offset += w_size\n",
        "        assert x_offset == len(x)\n",
        "\n",
        "    def _fun_generator(self, x, iterator):\n",
        "        \"\"\" Function optimized by scipy minimize.\n",
        "\n",
        "            Returns function cost and gradients for all trainable variables.\n",
        "        \"\"\"\n",
        "        model = self.model\n",
        "        self._update_weights(x)\n",
        "        losses = []\n",
        "\n",
        "        dataset = iterator._dataset  # pylint:disable=protected-access\n",
        "        assert dataset is not None\n",
        "        iterator = iter(dataset)\n",
        "\n",
        "        size = dataset.cardinality().numpy()\n",
        "        if size > 0:\n",
        "            n_steps = (size + dataset.batch_size - 1) // dataset.batch_size\n",
        "        else:\n",
        "            n_steps = None\n",
        "\n",
        "        progbar = keras.utils.Progbar(n_steps, verbose=self.verbose)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            for step, data in enumerate(iterator):\n",
        "                data = data_adapter.expand_1d(data)\n",
        "                x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(\n",
        "                    data)\n",
        "                y_pred = self.func(x, training=True)\n",
        "                loss = model.compiled_loss(y, y_pred, sample_weight,\n",
        "                                           regularization_losses=model.losses)\n",
        "                progbar.update(step, [('loss', loss.numpy())])\n",
        "                losses.append(loss)\n",
        "            xloss = tf.reduce_mean(tf.stack(losses))\n",
        "            grads = tape.gradient(xloss, model.trainable_variables)\n",
        "\n",
        "        cost = xloss.numpy()\n",
        "\n",
        "        if all(isinstance(x, tf.Tensor) for x in grads):\n",
        "            xgrads = np.concatenate([x.numpy().reshape(-1) for x in grads])\n",
        "            return cost, xgrads\n",
        "\n",
        "        if all(isinstance(x, tf.IndexedSlices) for x in grads):\n",
        "            xgrad_list = []\n",
        "            for var, grad in zip(model.trainable_variables, grads):\n",
        "                value = tf.Variable(np.zeros(var.shape), dtype=var.dtype)\n",
        "                value.assign_add(grad)\n",
        "                xgrad_list.append(value.numpy())\n",
        "            xgrads = np.concatenate([x.reshape(-1) for x in xgrad_list])\n",
        "            return cost, xgrads\n",
        "\n",
        "        raise NotImplementedError()\n",
        "        return -1, np.array([])  # pylint:disable=unreachable\n",
        "\n",
        "    def train_function(self, iterator):\n",
        "        \"\"\" Called by model fit.\n",
        "        \"\"\"\n",
        "        min_options = {\n",
        "            'maxiter': self.maxiter,\n",
        "            'disp': bool(self.verbose),\n",
        "        }\n",
        "\n",
        "        var_list = self.model.trainable_variables\n",
        "        x0 = np.concatenate([x.numpy().reshape(-1) for x in var_list])\n",
        "\n",
        "        result = minimize(\n",
        "            self._fun_generator, x0, method=self.method, jac=True,\n",
        "            options=min_options, args=(iterator,))\n",
        "\n",
        "        self._update_weights(result['x'])\n",
        "        return {'loss': result['fun']}\n",
        "\n",
        "\n",
        "def make_train_function(model, **kwargs):\n",
        "    \"\"\" Returns a function that will be called to train the model.\n",
        "\n",
        "        model._steps_per_execution must be set in order for train function to\n",
        "        be called once per epoch.\n",
        "    \"\"\"\n",
        "    model._assert_compile_was_called()  # pylint:disable=protected-access\n",
        "    model._configure_steps_per_execution(tf.int64.max)  # pylint:disable=protected-access\n",
        "    opt = ScipyOptimizer(model, **kwargs)\n",
        "    return opt.train_function"
      ],
      "metadata": {
        "id": "iTUXzpw8GzHh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iPZ7AMmXMNSB"
      },
      "outputs": [],
      "source": [
        "# set seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collocation points\n",
        "Ncl = 10000\n",
        "Xcl = lhs(2,Ncl)\n",
        "xcl = tf.expand_dims(tf.cast(-1.5+(3.0)*Xcl[:,0],dtype=tf.float64),axis=-1)\n",
        "ycl = tf.expand_dims(tf.cast(-1.5+(3.0)*Xcl[:,1],dtype=tf.float64),axis=-1)\n",
        "X_coll = tf.concat([xcl,ycl],1)"
      ],
      "metadata": {
        "id": "Io-jdXVYG2fq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def penalty(param, lower_bound, upper_bound):\n",
        "    return tf.reduce_sum(tf.square(tf.maximum(param - upper_bound, 0)) +\n",
        "                         tf.square(tf.maximum(lower_bound - param, 0)))\n",
        "# Residual loss\n",
        "@tf.function\n",
        "def r_PINN(x,y,param1, param2, param3):\n",
        "    input_data=tf.concat([x,y],1)\n",
        "    u = PINN_base(input_data)\n",
        "    u_x = tf.gradients(u,x)[0]\n",
        "    u_y = tf.gradients(u,y)[0]\n",
        "    u_grad = tf.transpose(tf.concat([u_x, u_y], axis=1))\n",
        "\n",
        "    pi = tf.constant(np.pi,dtype=tf.float64)\n",
        "    param1\n",
        "    theta0 = pi/2 - param1\n",
        "    a = tf.stack([tf.cos(theta0), tf.sin(theta0)])\n",
        "    b = tf.stack([tf.cos(theta0-pi/2), tf.sin(theta0-pi/2)])\n",
        "\n",
        "    D_00 = 1 / param2 * a[0]**2 + b[0]**2\n",
        "    D_01 = 1 / param2 * a[0] * a[1] + b[0] * b[1]\n",
        "    D_10 = 1 / param2 * a[0] * a[1] + b[0] * b[1]\n",
        "    D_11 = 1 / param2 * a[1]**2 + b[1]**2\n",
        "\n",
        "    return ((u_x * D_00 * u_x + u_x * D_01 * u_y + u_y * D_10 * u_x + u_y * D_11 * u_y))  - 1/100**2\n",
        "\n",
        "# PINN loss function\n",
        "def my_loss_fn(y_true, y_pred, xcl = xcl, ycl = ycl):\n",
        "\n",
        "    param1 = tf.reduce_mean(y_pred[:,1])\n",
        "    param2 = tf.reduce_mean(y_pred[:,2])\n",
        "    param3 = tf.reduce_mean(y_pred[:,3])\n",
        "    r_pred = r_PINN(xcl,ycl,param1, param2, param3)\n",
        "    \"\"\"\n",
        "    tf.print(param1)\n",
        "    tf.print(param2)\n",
        "    tf.print(param3)\n",
        "    \"\"\"\n",
        "    # loss components\n",
        "    mse_meas = tf.reduce_mean(tf.pow( y_true[:,0]-y_pred[:,0], 2 ))\n",
        "    mse_r  = tf.reduce_mean(tf.abs(r_pred))\n",
        "\n",
        "    # bc\n",
        "    p1_tensor = tf.expand_dims(tf.convert_to_tensor(param1, dtype=tf.float64), axis=0)\n",
        "    p2_tensor = tf.expand_dims(tf.convert_to_tensor(param2, dtype=tf.float64), axis=0)\n",
        "    p3_tensor = tf.expand_dims(tf.convert_to_tensor(param3, dtype=tf.float64), axis=0)\n",
        "    origin = tf.expand_dims(tf.convert_to_tensor([1.5, param3], dtype=tf.float64), axis=0)\n",
        "\n",
        "    mse_bc = tf.pow(PINN([origin, p1_tensor, p2_tensor, p3_tensor])[:,0], 2)\n",
        "\n",
        "    #penalty over param boundaries\n",
        "    mse_penalty = penalty(param1,-np.pi/10,np.pi/10)+penalty(param2,1,9)+penalty(param3,-1.5,1.5)\n",
        "\n",
        "    return mse_meas + mse_r + mse_bc + mse_penalty"
      ],
      "metadata": {
        "id": "JTqog6-gGQP4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "regularization_strength = 1e-3\n",
        "\n",
        "PINN_base = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation='relu', input_shape=(2,),\n",
        "                          kernel_initializer=\"glorot_uniform\",\n",
        "                          kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                          dtype=tf.float64),\n",
        "\n",
        "    #tf.keras.layers.Reshape((1, 32)),\n",
        "\n",
        "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64)),\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation='relu',\n",
        "                          kernel_initializer=\"glorot_uniform\",\n",
        "                          kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                          dtype=tf.float64),\n",
        "\n",
        "    tf.keras.layers.Dense(64, activation='relu',\n",
        "                          kernel_initializer=\"glorot_uniform\",\n",
        "                          kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                          dtype=tf.float64),\n",
        "\n",
        "    tf.keras.layers.Dense(1, activation=None,\n",
        "                          kernel_initializer=\"glorot_uniform\",\n",
        "                          kernel_regularizer=regularizers.l2(regularization_strength),\n",
        "                          dtype=tf.float64)\n",
        "])"
      ],
      "metadata": {
        "id": "9mAnhAExG7HO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import RBFInterpolator\n",
        "\n",
        "def checkpoint1_solution(x, y, t, X, Y, s_value=0.05, s_aniso_1=0.5, s_aniso_2=0.5):\n",
        "    coordinates = np.column_stack((x, y))\n",
        "\n",
        "    mesh_coordinates=np.column_stack((X.ravel(), Y.ravel()))\n",
        "\n",
        "    s = [s_value,s_value,s_value,s_value,s_value,s_value,s_value,s_value,s_value,s_value,s_aniso_1, s_value,s_value,s_value,s_value, s_aniso_2,s_value,s_value,s_value,s_value]\n",
        "\n",
        "    rbf = RBFInterpolator(coordinates, t, neighbors=None, smoothing=s, kernel='thin_plate_spline', epsilon=None, degree=1)\n",
        "\n",
        "    time_pred = rbf(mesh_coordinates)\n",
        "    time_pred=time_pred.reshape(1501,1501)\n",
        "\n",
        "    return time_pred"
      ],
      "metadata": {
        "id": "HAr5JYX2HBEY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ind_disp = 0\n",
        "x = CP2data[ind_disp][0]\n",
        "y = CP2data[ind_disp][1]\n",
        "t = CP2data[ind_disp][2]"
      ],
      "metadata": {
        "id": "5XATRvWOMTFg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_bias_initializer_theta(shape, dtype=None):\n",
        "    return tf.constant(0.01, shape = shape, dtype=dtype)\n",
        "\n",
        "def custom_bias_initializer_a_aniso(shape, dtype=None):\n",
        "    return tf.constant(4, shape = shape, dtype=dtype)\n",
        "\n",
        "def custom_bias_initializer_y0(shape, dtype=None):\n",
        "    return tf.constant(y0_initial, shape = shape, dtype=dtype)"
      ],
      "metadata": {
        "id": "toL-nZaUeSYX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train/val split\n",
        "xmeas_train, xmeas_val, ymeas_train, ymeas_val, tmeas_train, tmeas_val = train_test_split(x, y, t, test_size=0.1)\n",
        "xmeas_train = tf.constant(xmeas_train.reshape(18, 1), dtype=tf.float64)\n",
        "ymeas_train = tf.constant(ymeas_train.reshape(18, 1), dtype=tf.float64)\n",
        "tmeas_train = tf.constant(tmeas_train.reshape(18, 1), dtype=tf.float64)\n",
        "xmeas_val = tf.constant(xmeas_val.reshape(2, 1), dtype=tf.float64)\n",
        "ymeas_val = tf.constant(ymeas_val.reshape(2, 1), dtype=tf.float64)\n",
        "tmeas_val = tf.constant(tmeas_val.reshape(2, 1), dtype=tf.float64)\n",
        "\n",
        "# y0 initial guess\n",
        "X, Y = np.meshgrid(np.linspace(-1.5,1.5,1501), np.linspace(-1.5,1.5,1501))\n",
        "time_pred = checkpoint1_solution(x, y, t, X, Y, s_value=0.05, s_aniso_1=0.5, s_aniso_2=0.5)\n",
        "y0_initial = Y[np.where(time_pred==np.min(time_pred))]\n",
        "\n",
        "# parameter is treated as the bias of an additional layer that always receives a zero input.\n",
        "\n",
        "inputs = tf.keras.Input(shape=(2,))\n",
        "input1 = tf.keras.Input(shape=(1,), dtype=tf.float64)\n",
        "input2 = tf.keras.Input(shape=(1,), dtype=tf.float64)\n",
        "input3 = tf.keras.Input(shape=(1,), dtype=tf.float64)\n",
        "emb1 = tf.keras.layers.Dense(1, input_shape=(1,), bias_initializer = custom_bias_initializer_theta, dtype=tf.float64)\n",
        "emb2 = tf.keras.layers.Dense(1, input_shape=(1,), bias_initializer = custom_bias_initializer_a_aniso, dtype=tf.float64)\n",
        "emb3 = tf.keras.layers.Dense(1, input_shape=(1,), bias_initializer = custom_bias_initializer_y0, dtype=tf.float64)\n",
        "output = tf.concat( [PINN_base(inputs), emb1(input1), emb2(input2), emb3(input3) ] , 1 )\n",
        "\n",
        "PINN = tf.keras.Model(inputs=[inputs,input1,input2,input3], outputs=output)\n",
        "\n",
        "PINN.compile(loss = my_loss_fn, optimizer=tf.keras.optimizers.Adam(learning_rate=0.002,beta_1=0.99))\n",
        "\n",
        "history_adam = PINN.fit( [ tf.concat([xmeas_train,ymeas_train],1), tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train)) ], tmeas_train , epochs=120)\n",
        "\n",
        "PINN.compile(loss = my_loss_fn)\n",
        "\n",
        "PINN.train_function = make_train_function(PINN, maxiter=5)\n",
        "\n",
        "history = PINN.fit( [ tf.concat([xmeas_train,ymeas_train],1) , tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train)) ], tmeas_train)\n",
        "\n",
        "param = tf.reduce_mean(PINN([tf.concat([xmeas_train,ymeas_train],1) , tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train))])[:,1])\n",
        "print(param)\n",
        "\"\"\"\n",
        "# Adam optimizer\n",
        "initial_learning_rate = 0.002\n",
        "tf_optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate,beta_1=0.99)\n",
        "\n",
        "patience = float('inf')\n",
        "patience_lr = 500\n",
        "min_delta = 1e-9\n",
        "best_val_loss = float('inf')\n",
        "wait = 0\n",
        "count = 0\n",
        "\n",
        "for iter in range(12000):\n",
        "\n",
        "  # compute gradients using AD\n",
        "  loss_value,grads,grad_param = grad(PINN,xcl,ycl,xmeas_train, ymeas_train, tmeas_train, param)\n",
        "\n",
        "  # update neural network weights\n",
        "  tf_optimizer.apply_gradients(zip(grads+[grad_param],PINN.trainable_variables+[param]))\n",
        "\n",
        "  loss_value_val, _, _ = grad(PINN, xcl, ycl, xmeas_val, ymeas_val, tmeas_val, param)\n",
        "\n",
        "  best_weigths = None\n",
        "  best_params = None\n",
        "\n",
        "  # Early stopping\n",
        "  if loss_value_val < best_val_loss - min_delta:\n",
        "      best_val_loss = loss_value_val\n",
        "      wait = 0\n",
        "      count = 0\n",
        "      best_weights = PINN.get_weights()\n",
        "      best_params = param.numpy()\n",
        "  else:\n",
        "      wait += 1\n",
        "      count += 1\n",
        "\n",
        "      if count >= patience_lr:\n",
        "        tf_optimizer.learning_rate = tf_optimizer.learning_rate * 0.9\n",
        "        count = 0\n",
        "\n",
        "      if wait >= patience:\n",
        "          print('Early stopping at epoch', iter + 1)\n",
        "          break\n",
        "\n",
        "  # display intermediate results\n",
        "  if ((iter+1) % 100 == 0):\n",
        "    print('iter =  '+str(iter+1))\n",
        "    #loss_value_np=loss_value.numpy()\n",
        "    #print('loss = {:.4f}'.format(loss_value_np))\n",
        "    tf.print('loss =' , loss_value)\n",
        "    tf.print('loss_val_param =' , loss_value_val)\n",
        "\n",
        "    print(param.numpy())\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "AMtbM_TYGxHl",
        "outputId": "d12815ec-d45b-4622-ce0e-7ab95b812b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "1/1 [==============================] - 12s 12s/step - loss: 0.1797\n",
            "Epoch 2/120\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.1440\n",
            "Epoch 3/120\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.1515\n",
            "Epoch 4/120\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.1523\n",
            "Epoch 5/120\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.1429\n",
            "Epoch 6/120\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1337\n",
            "Epoch 7/120\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.1288\n",
            "Epoch 8/120\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.1285\n",
            "Epoch 9/120\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.1295\n",
            "Epoch 10/120\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1292\n",
            "Epoch 11/120\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1262\n",
            "Epoch 12/120\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.1210\n",
            "Epoch 13/120\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.1157\n",
            "Epoch 14/120\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1115\n",
            "Epoch 15/120\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.1090\n",
            "Epoch 16/120\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.1082\n",
            "Epoch 17/120\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.1078\n",
            "Epoch 18/120\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.1072\n",
            "Epoch 19/120\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.1051\n",
            "Epoch 20/120\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1021\n",
            "Epoch 21/120\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0987\n",
            "Epoch 22/120\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0953\n",
            "Epoch 23/120\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0922\n",
            "Epoch 24/120\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0898\n",
            "Epoch 25/120\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0879\n",
            "Epoch 26/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0864\n",
            "Epoch 27/120\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0853\n",
            "Epoch 28/120\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0842\n",
            "Epoch 29/120\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0830\n",
            "Epoch 30/120\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.0815\n",
            "Epoch 31/120\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0797\n",
            "Epoch 32/120\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0777\n",
            "Epoch 33/120\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0755\n",
            "Epoch 34/120\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0733\n",
            "Epoch 35/120\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0712\n",
            "Epoch 36/120\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0694\n",
            "Epoch 37/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0678\n",
            "Epoch 38/120\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0666\n",
            "Epoch 39/120\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0656\n",
            "Epoch 40/120\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0647\n",
            "Epoch 41/120\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0638\n",
            "Epoch 42/120\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0627\n",
            "Epoch 43/120\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0616\n",
            "Epoch 44/120\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0603\n",
            "Epoch 45/120\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0589\n",
            "Epoch 46/120\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0574\n",
            "Epoch 47/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0559\n",
            "Epoch 48/120\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0545\n",
            "Epoch 49/120\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0532\n",
            "Epoch 50/120\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0520\n",
            "Epoch 51/120\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0509\n",
            "Epoch 52/120\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0500\n",
            "Epoch 53/120\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0492\n",
            "Epoch 54/120\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0485\n",
            "Epoch 55/120\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0479\n",
            "Epoch 56/120\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0472\n",
            "Epoch 57/120\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0466\n",
            "Epoch 58/120\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0458\n",
            "Epoch 59/120\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.0450\n",
            "Epoch 60/120\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0442\n",
            "Epoch 61/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0433\n",
            "Epoch 62/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0424\n",
            "Epoch 63/120\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0416\n",
            "Epoch 64/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0408\n",
            "Epoch 65/120\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0402\n",
            "Epoch 66/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0397\n",
            "Epoch 67/120\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0392\n",
            "Epoch 68/120\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0388\n",
            "Epoch 69/120\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0384\n",
            "Epoch 70/120\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0379\n",
            "Epoch 71/120\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0375\n",
            "Epoch 72/120\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0370\n",
            "Epoch 73/120\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0364\n",
            "Epoch 74/120\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0359\n",
            "Epoch 75/120\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0354\n",
            "Epoch 76/120\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0349\n",
            "Epoch 77/120\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0344\n",
            "Epoch 78/120\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0340\n",
            "Epoch 79/120\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0336\n",
            "Epoch 80/120\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0333\n",
            "Epoch 81/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0331\n",
            "Epoch 82/120\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0328\n",
            "Epoch 83/120\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0326\n",
            "Epoch 84/120\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0323\n",
            "Epoch 85/120\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0321\n",
            "Epoch 86/120\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0318\n",
            "Epoch 87/120\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0316\n",
            "Epoch 88/120\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.0313\n",
            "Epoch 89/120\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0310\n",
            "Epoch 90/120\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.0308\n",
            "Epoch 91/120\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0306\n",
            "Epoch 92/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0304\n",
            "Epoch 93/120\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0302\n",
            "Epoch 94/120\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0300\n",
            "Epoch 95/120\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0299\n",
            "Epoch 96/120\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0298\n",
            "Epoch 97/120\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0297\n",
            "Epoch 98/120\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.0295\n",
            "Epoch 99/120\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0294\n",
            "Epoch 100/120\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.0293\n",
            "Epoch 101/120\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0291\n",
            "Epoch 102/120\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0290\n",
            "Epoch 103/120\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.0288\n",
            "Epoch 104/120\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.0287\n",
            "Epoch 105/120\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.0286\n",
            "Epoch 106/120\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.0285\n",
            "Epoch 107/120\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.0284\n",
            "Epoch 108/120\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0284\n",
            "Epoch 109/120\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0283\n",
            "Epoch 110/120\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0282\n",
            "Epoch 111/120\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0282\n",
            "Epoch 112/120\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.0281\n",
            "Epoch 113/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0280\n",
            "Epoch 114/120\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.0279\n",
            "Epoch 115/120\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.0278\n",
            "Epoch 116/120\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.0277\n",
            "Epoch 117/120\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.0277\n",
            "Epoch 118/120\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.0276\n",
            "Epoch 119/120\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.0275\n",
            "Epoch 120/120\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.0274\n",
            "      0/Unknown - 0s 0s/step - loss: 0.0015         Current function value: 0.001484\n",
            "         Iterations: 5\n",
            "         Function evaluations: 10\n",
            "         Gradient evaluations: 10\n",
            "1/1 [==============================] - 1078s 1078s/step - loss: 0.0208\n",
            "tf.Tensor(0.14161838248267763, shape=(), dtype=float64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py:705: OptimizeWarning: Maximum number of iterations has been exceeded.\n",
            "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Adam optimizer\\ninitial_learning_rate = 0.002\\ntf_optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate,beta_1=0.99)\\n\\npatience = float('inf')\\npatience_lr = 500\\nmin_delta = 1e-9\\nbest_val_loss = float('inf')\\nwait = 0\\ncount = 0\\n\\nfor iter in range(12000):\\n\\n  # compute gradients using AD\\n  loss_value,grads,grad_param = grad(PINN,xcl,ycl,xmeas_train, ymeas_train, tmeas_train, param)\\n\\n  # update neural network weights\\n  tf_optimizer.apply_gradients(zip(grads+[grad_param],PINN.trainable_variables+[param]))\\n\\n  loss_value_val, _, _ = grad(PINN, xcl, ycl, xmeas_val, ymeas_val, tmeas_val, param)\\n\\n  best_weigths = None\\n  best_params = None\\n\\n  # Early stopping\\n  if loss_value_val < best_val_loss - min_delta:\\n      best_val_loss = loss_value_val\\n      wait = 0\\n      count = 0\\n      best_weights = PINN.get_weights()\\n      best_params = param.numpy()\\n  else:\\n      wait += 1\\n      count += 1\\n\\n      if count >= patience_lr:\\n        tf_optimizer.learning_rate = tf_optimizer.learning_rate * 0.9\\n        count = 0\\n\\n      if wait >= patience:\\n          print('Early stopping at epoch', iter + 1)\\n          break\\n\\n  # display intermediate results\\n  if ((iter+1) % 100 == 0):\\n    print('iter =  '+str(iter+1))\\n    #loss_value_np=loss_value.numpy()\\n    #print('loss = {:.4f}'.format(loss_value_np))\\n    tf.print('loss =' , loss_value)\\n    tf.print('loss_val_param =' , loss_value_val)\\n\\n    print(param.numpy())\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param1 = tf.reduce_mean(PINN([tf.concat([xmeas_train,ymeas_train],1) , tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train))])[:,1])\n",
        "param2 = tf.reduce_mean(PINN([tf.concat([xmeas_train,ymeas_train],1) , tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train))])[:,2])\n",
        "param3 = tf.reduce_mean(PINN([tf.concat([xmeas_train,ymeas_train],1) , tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train)), tf.zeros(tf.shape(xmeas_train))])[:,3])\n",
        "print(param1, param2, param3)"
      ],
      "metadata": {
        "id": "EK102IDX1_2E",
        "outputId": "1c3e5d7b-94a6-4166-d195-8dca20dc493d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.14161838248267763, shape=(), dtype=float64) tf.Tensor(4.144833637166823, shape=(), dtype=float64) tf.Tensor(0.5775586228917283, shape=(), dtype=float64)\n"
          ]
        }
      ]
    }
  ]
}