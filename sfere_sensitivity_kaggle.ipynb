{"metadata":{"colab":{"provenance":[],"gpuType":"T4","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Riky2014/NAPDE/blob/main/sfere_sensitivity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"# Install","metadata":{"id":"UHWLBC4QPixK"}},{"cell_type":"code","source":"%%capture\n!pip install -U \"monai-weekly[fire, nibabel, yaml, tqdm, einops]\"","metadata":{"execution":{"iopub.execute_input":"2024-02-29T11:00:07.656855Z","iopub.status.busy":"2024-02-29T11:00:07.656289Z","iopub.status.idle":"2024-02-29T11:00:40.779574Z","shell.execute_reply":"2024-02-29T11:00:40.778439Z","shell.execute_reply.started":"2024-02-29T11:00:07.656827Z"},"id":"rW2wnoKqvzOG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mount Drive","metadata":{"id":"WEPirjF-N9-i"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EREYLLIN8cy","outputId":"2d6468b5-bc33-4cb4-bc92-863939e8fc5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/drive\n"}]},{"cell_type":"markdown","source":"# Import and set directory","metadata":{"id":"Cc63EQ-qZP7y"}},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport tempfile\nimport numpy as np\nimport nibabel as nib\nfrom PIL import Image\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom nibabel import load, save, Nifti1Image\n\nimport monai\nfrom monai.losses import DiceLoss\nfrom monai.metrics import DiceMetric\nfrom monai.utils import set_determinism\nfrom monai.networks.nets import SegResNet\nfrom monai.data import DataLoader, decollate_batch, create_test_image_3d\n\nfrom monai.transforms import (\n    Activations,\n    AsDiscrete,\n    Compose,\n    LoadImaged,\n    NormalizeIntensityd,\n    Orientationd,\n    RandFlipd,\n    RandScaleIntensityd,\n    RandShiftIntensityd,\n    RandSpatialCropd,\n    Spacingd,\n    EnsureTyped,\n    EnsureChannelFirstd,\n)","metadata":{"execution":{"iopub.execute_input":"2024-02-29T11:25:20.877099Z","iopub.status.busy":"2024-02-29T11:25:20.876430Z","iopub.status.idle":"2024-02-29T11:25:49.893062Z","shell.execute_reply":"2024-02-29T11:25:49.892119Z","shell.execute_reply.started":"2024-02-29T11:25:20.877065Z"},"id":"7u-pNMaGwAuN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory_path = '/kaggle/working/sfere_sensitivity'\nos.makedirs(directory_path, exist_ok = True)\nos.environ[\"MONAI_DATA_DIRECTORY\"] = directory_path\ndirectory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\nroot_dir = tempfile.mkdtemp() if directory is None else directory","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function definition","metadata":{"id":"1RQI7fJS2iPX"}},{"cell_type":"code","source":"def create_image_3d(n_train, n_test, noise, r_min, r_max, seed):\n    np.random.seed(seed)\n\n  for i in range(n_train + n_test):\n    im, seg = create_test_image_3d(128, 128, 128, num_seg_classes = 1, noise_max = noise, rad_min = r_min, rad_max = r_max)\n    n = nib.Nifti1Image(im, np.eye(4))\n    nib.save(n, os.path.join(root_dir, f\"image{i}.nii\"))\n    n = nib.Nifti1Image(seg, np.eye(4))\n    nib.save(n, os.path.join(root_dir, f\"label{i}.nii\"))\n\n  set_determinism(seed=0)\n\n  images = sorted(glob(os.path.join(root_dir, \"image*.nii\")))\n  labels = sorted(glob(os.path.join(root_dir, \"label*.nii\")))\n\n  train_files = [{\"image\": image, \"label\": label} for image, label in zip(images[:n_train], labels[:n_train])]\n  val_files = [{\"image\": image, \"label\": label} for image, label in zip(images[-n_test:], labels[-n_test:])]\n\n  return train_files, val_files","metadata":{"id":"Qn3uFhow2fhL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform(train_files, val_files):\n  train_transform = Compose([\n      LoadImaged(keys=[\"image\", \"label\"]),\n      EnsureChannelFirstd(keys=[\"image\",\"label\"]),\n      EnsureTyped(keys=[\"image\", \"label\"]),\n      Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n      Spacingd(\n          keys=[\"image\", \"label\"],\n          pixdim=(1.0, 1.0, 1.0),\n          mode=(\"bilinear\", \"nearest\"),\n      ),\n      RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[128, 128, 128], random_size=False),\n      RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n      RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n      RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n      NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n      RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n      RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n  ])\n\n  val_transform = Compose([\n      LoadImaged(keys=[\"image\", \"label\"]),\n      EnsureChannelFirstd(keys=[\"image\",\"label\"]),\n      EnsureTyped(keys=[\"image\", \"label\"]),\n      Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n      Spacingd(\n          keys=[\"image\", \"label\"],\n          pixdim=(1.0, 1.0, 1.0),\n          mode=(\"bilinear\", \"nearest\"),\n      ),\n      NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n  ])\n\n  train_ds = monai.data.Dataset(data=train_files, transform=train_transform)\n  train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=2)\n\n  val_ds = monai.data.Dataset(data=val_files, transform=val_transform)\n  val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n\n  return train_loader, val_loader, val_ds","metadata":{"id":"6EB_QmNK25uR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_and_train(train_loader, val_loader, max_epochs, val_ds):\n  val_interval = 1\n  VAL_AMP = True\n\n  device = torch.device(\"cuda:0\")\n  model = SegResNet(\n    blocks_down=[1, 2, 2, 4],\n    blocks_up=[1, 1, 1],\n    init_filters=16,\n    in_channels=1,\n    out_channels=1,\n    dropout_prob=0.2,\n  ).to(device)\n\n  loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n  optimizer = torch.optim.Adam(model.parameters(), 1e-3, weight_decay=1e-6)\n  lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n\n  dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n  dice_metric_train = DiceMetric(include_background=True, reduction=\"mean\")\n\n  post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n\n  scaler = torch.cuda.amp.GradScaler()\n  torch.backends.cudnn.benchmark = True\n\n  epoch_loss_values = []\n  metric_values = []\n  metric_values_train = []\n  total_start = time.time()\n\n  for epoch in range(max_epochs):\n    epoch_start = time.time()\n    print(\"-\" * 10)\n    print(f\"epoch {epoch + 1}/{max_epochs}\")\n    model.train()\n    epoch_loss = 0\n    step = 0\n    for batch_data in train_loader:\n        step_start = time.time()\n        step += 1\n        inputs, labels = ( batch_data[\"image\"].to(device), batch_data[\"label\"].to(device) )\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n            outputs = model(inputs)\n            loss = loss_function(outputs, labels)\n\n        outputs = [post_trans(i) for i in decollate_batch(outputs)]\n\n        dice_metric_train(y_pred=outputs, y=labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        epoch_loss += loss.item()\n\n    metric_train = dice_metric_train.aggregate().item()\n    metric_values_train.append(metric_train)\n    dice_metric_train.reset()\n\n    lr_scheduler.step()\n    epoch_loss /= step\n    epoch_loss_values.append(epoch_loss)\n    print(f\"Loss: {epoch_loss:.4f} \\nTrain dice: {metric_train:.4f}\")\n\n    if (epoch + 1) % val_interval == 0:\n        model.eval()\n        with torch.no_grad():\n            for val_data in val_loader:\n                val_inputs, val_labels = (val_data[\"image\"].to(device),val_data[\"label\"].to(device))\n\n                val_outputs = model(val_inputs)\n                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n\n                dice_metric(y_pred=val_outputs, y=val_labels)\n\n            metric = dice_metric.aggregate().item()\n            metric_values.append(metric)\n            dice_metric.reset()\n\n            print(f\"Test dice: {metric:.4f}\")\n\n    print(f\"Time: {(time.time() - epoch_start):.4f}\")\n  total_time = time.time() - total_start\n  print(f\"Train completed, total time: {total_time}.\")\n  print()\n  print(f\"Train metric = {metric_values_train[-1]}, Test metric = {metric_values[-1]}\")\n  print()\n  print()\n\n  model.eval()\n  with torch.no_grad():\n    i = 0\n\n    val_input = val_ds[i][\"image\"].unsqueeze(0).to(device)\n\n    val_output = model(val_input)\n    val_output = post_trans(val_output[0])\n\n    plt.figure(\"fig\")\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12,6))\n\n    ax1.set_title(\"Image slice\")\n    ax1.imshow(val_ds[i][\"image\"][0, :, :, 64].detach().cpu(), cmap=\"gray\")\n\n    ax2.set_title(\"Label slice\")\n    ax2.imshow(val_ds[i][\"label\"][0, :, :, 64].detach().cpu())\n\n    ax3.set_title(\"Output slice\")\n    ax3.imshow(val_output[0, :, :, 64].detach().cpu())\n\n    plt.show()\n\n    fig, ax = plt.subplots(1, 2, subplot_kw={\"projection\": \"3d\"},figsize = (12,6))\n\n    z, x, y = val_ds[i][\"label\"][0].astype(np.uint8).nonzero()\n    ax[0].scatter(x, y, z)\n    ax[0].set_xlim([0,128])\n    ax[0].set_ylim([0,128])\n    ax[0].set_title(\"Label\")\n\n    z, x, y = val_output[0].astype(np.uint8).nonzero()\n    ax[1].scatter(x, y, z)\n    ax[1].set_xlim([0,128])\n    ax[1].set_ylim([0,128])\n    ax[1].set_title(\"Output\")\n\n    plt.show()\n\n  return metric_values_train[-1], metric_values[-1], epoch_loss_values[-1]","metadata":{"id":"moXDkv0d3biq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_train_dim(n_train_vec, metric_train, metric_test, loss, r_min, r_max, noise, max_epochs):\n  output_dir = '/kaggle/working/output_plots'\n  os.makedirs(output_dir, exist_ok=True)\n  plot_path = os.path.join(output_dir, f\"2D, noise = {noise}, epochs = {max_epochs}, r = ({r_min}, {r_max}).png\")\n\n  fig, ax = plt.subplots(1, 2, figsize = (12,6))\n\n  plt.subplot(1,2,1)\n  plt.plot(n_train_vec, metric_train, label = 'Train metric', marker='*')\n  plt.plot(n_train_vec, metric_test, label = 'Test metric', marker='*')\n  plt.xscale('log', base = 2)\n  plt.xlabel(\"Train dimension\")\n  plt.title(f\"3D, noise = {noise}, epochs = {max_epochs}, r = ({r_min}, {r_max})\")\n  plt.legend()\n\n  plt.subplot(1,2,2)\n  plt.plot(n_train_vec, loss, label = 'Train loss', marker='*', color='red')\n  plt.xscale('log', base = 2)\n  plt.xlabel(\"Train dimension\")\n  plt.title(f\"3D, noise = {noise}, epochs = {max_epochs}, r = ({r_min}, {r_max})\")\n  plt.legend()\n\n  plt.savefig(plot_path)","metadata":{"id":"VQgE1cFd4iqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_noise(noise_vec, metric_train, metric_test, loss, r_min, r_max, max_epochs):\n    \n  output_dir = '/kaggle/working/output_plots'\n  os.makedirs(output_dir, exist_ok=True)\n  plot_path = os.path.join(output_dir, '2D, epochs = {max_epochs}, r = ({r_min}, {r_max}).png')\n\n  fig, ax = plt.subplots(1, 2, figsize = (12,6))\n\n  plt.subplot(1,2,1)\n  plt.plot(noise_vec, metric_train, label = 'Train metric', marker='*')\n  plt.plot(noise_vec, metric_test, label = 'Test metric', marker='*')\n  plt.xlabel(\"Noise\")\n  plt.title(f\"3D, epochs = {max_epochs}, r = ({r_min}, {r_max})\")\n  plt.legend()\n\n  plt.subplot(1,2,2)\n  plt.plot(noise_vec, loss, label = 'Train loss', marker='*', color='red')\n  plt.xlabel(\"Noise\")\n  plt.title(f\"3D, epochs = {max_epochs}, r = ({r_min}, {r_max})\")\n  plt.legend()\n\n  plt.savefig(plot_path)","metadata":{"id":"lwi_Gcx6xGJK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def delete_image(directory_path):\n  file_list = os.listdir(directory_path)\n  for file in file_list:\n      file_path = os.path.join(directory_path, file)\n      if os.path.isfile(file_path):\n          os.remove(file_path)\n      elif os.path.isdir(file_path):\n          os.rmdir(file_path)\n  print()","metadata":{"id":"eg5tVBl24vDi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Execute training\nSpecify:\n- minimum radius (\"r_min\": int)\n- maximum radius (\"r_max\": int)\n- noise to be added (\"noise\": float)\n- training epochs (\"max_epochs\": int)","metadata":{"id":"u7sSU8mnGZVb"}},{"cell_type":"markdown","source":"## Dimention train samples","metadata":{"id":"dfKHZqqHxIud"}},{"cell_type":"code","source":"n_train_vec  = [8, 16, 32, 64, 128]\nn_test = 30\n\nnoise = 1\nr_min = 5\nr_max = 10\n\nmax_epochs = 25\n\nmetric_values_train_vec = []\nmetric_values_vec = []\nepoch_loss_values_vec = []\n\n\nfor n_train in n_train_vec:\n  print()\n  print(f\"Number of training images = {n_train}\")\n  print(f\"Number of testing images = {n_test}\")\n  print()\n\n  train_files, val_files = create_image_3d(n_train, n_test, noise, r_min, r_max)\n  train_loader, val_loader, val_ds = transform(train_files, val_files)\n\n  metric_value_train, metric_value, epoch_loss_value = model_and_train(train_loader, val_loader, max_epochs, val_ds)\n\n  metric_values_train_vec.append(metric_value_train)\n  metric_values_vec.append(metric_value)\n  epoch_loss_values_vec.append(epoch_loss_value)\n\n  delete_image(directory_path)","metadata":{"id":"YKLf6jLuGcKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_train_dim(n_train_vec, metric_values_train_vec, metric_values_vec, epoch_loss_values_vec, r_min, r_max, noise, max_epochs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Noise","metadata":{"id":"uTAb4JhyTSxt"}},{"cell_type":"code","source":"n_train = 40\nn_test = 30\n\nnoise_vec = [0, 0.5, 1, 2, 3, 4, 5, 6, 7]\nr_min = 5\nr_max = 10\n\nmax_epochs = 25\n\nmetric_values_train_vec = []\nmetric_values_vec = []\nepoch_loss_values_vec = []\nqaa\n\nfor noise in noise_vec:\n  print()\n  print(f\"Noise = {noise}\")\n  print()\n\n  train_files, val_files = create_image_3d(n_train, n_test, noise, r_min, r_max, int(10 * noise))\n  train_loader, val_loader, val_ds = transform(train_files, val_files)\n\n  metric_value_train, metric_value, epoch_loss_value = model_and_train(train_loader, val_loader, max_epochs, val_ds)\n\n  metric_values_train_vec.append(metric_value_train)\n  metric_values_vec.append(metric_value)\n  epoch_loss_values_vec.append(epoch_loss_value)\n\n  delete_image(directory_path)","metadata":{"id":"jA43B-3QSR5j","outputId":"78badf96-606b-4ef7-9d2d-82ee2a461b72","colab":{"base_uri":"https://localhost:8080/","height":1000}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_noise(noise_vec, metric_values_train_vec, metric_values_vec, epoch_loss_values_vec, r_min, r_max, max_epochs)","metadata":{},"execution_count":null,"outputs":[]}]}