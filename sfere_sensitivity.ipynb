{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riky2014/NAPDE/blob/main/sfere_sensitivity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHWLBC4QPixK"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-29T11:00:07.656855Z",
          "iopub.status.busy": "2024-02-29T11:00:07.656289Z",
          "iopub.status.idle": "2024-02-29T11:00:40.779574Z",
          "shell.execute_reply": "2024-02-29T11:00:40.778439Z",
          "shell.execute_reply.started": "2024-02-29T11:00:07.656827Z"
        },
        "id": "rW2wnoKqvzOG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U \"monai-weekly[fire, nibabel, yaml, tqdm, einops]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEPirjF-N9-i"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EREYLLIN8cy",
        "outputId": "2d6468b5-bc33-4cb4-bc92-863939e8fc5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc63EQ-qZP7y"
      },
      "source": [
        "# Import and set directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-29T11:25:20.877099Z",
          "iopub.status.busy": "2024-02-29T11:25:20.876430Z",
          "iopub.status.idle": "2024-02-29T11:25:49.893062Z",
          "shell.execute_reply": "2024-02-29T11:25:49.892119Z",
          "shell.execute_reply.started": "2024-02-29T11:25:20.877065Z"
        },
        "id": "7u-pNMaGwAuN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from nibabel import load, save, Nifti1Image\n",
        "\n",
        "import monai\n",
        "from monai.losses import DiceLoss\n",
        "from monai.metrics import DiceMetric\n",
        "from monai.utils import set_determinism\n",
        "from monai.networks.nets import SegResNet\n",
        "from monai.data import DataLoader, decollate_batch, create_test_image_3d\n",
        "\n",
        "from monai.transforms import (\n",
        "    Activations,\n",
        "    AsDiscrete,\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    NormalizeIntensityd,\n",
        "    Orientationd,\n",
        "    RandFlipd,\n",
        "    RandScaleIntensityd,\n",
        "    RandShiftIntensityd,\n",
        "    RandSpatialCropd,\n",
        "    Spacingd,\n",
        "    EnsureTyped,\n",
        "    EnsureChannelFirstd,\n",
        ")\n",
        "\n",
        "directory_path = '/content/drive/MyDrive/sfere_sensitivity'\n",
        "os.environ[\"MONAI_DATA_DIRECTORY\"] = directory_path\n",
        "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
        "root_dir = tempfile.mkdtemp() if directory is None else directory"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function definition"
      ],
      "metadata": {
        "id": "1RQI7fJS2iPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_image_3d(n_train, n_test, noise, r_min, r_max):\n",
        "\n",
        "  for i in range(n_train + n_test):\n",
        "    im, seg = create_test_image_3d(128, 128, 128, num_seg_classes = 1, noise_max = noise, rad_min = r_min, rad_max = r_max)\n",
        "    n = nib.Nifti1Image(im, np.eye(4))\n",
        "    nib.save(n, os.path.join(root_dir, f\"image{i}.nii\"))\n",
        "    n = nib.Nifti1Image(seg, np.eye(4))\n",
        "    nib.save(n, os.path.join(root_dir, f\"label{i}.nii\"))\n",
        "\n",
        "  set_determinism(seed=0)\n",
        "\n",
        "  images = sorted(glob(os.path.join(root_dir, \"image*.nii\")))\n",
        "  labels = sorted(glob(os.path.join(root_dir, \"label*.nii\")))\n",
        "\n",
        "  train_files = [{\"image\": image, \"label\": label} for image, label in zip(images[:n_train], labels[:n_train])]\n",
        "  val_files = [{\"image\": image, \"label\": label} for image, label in zip(images[-n_test:], labels[-n_test:])]\n",
        "\n",
        "  return train_files, val_files"
      ],
      "metadata": {
        "id": "Qn3uFhow2fhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(train_files, val_files):\n",
        "  train_transform = Compose([\n",
        "      LoadImaged(keys=[\"image\", \"label\"]),\n",
        "      EnsureChannelFirstd(keys=[\"image\",\"label\"]),\n",
        "      EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "      Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "      Spacingd(\n",
        "          keys=[\"image\", \"label\"],\n",
        "          pixdim=(1.0, 1.0, 1.0),\n",
        "          mode=(\"bilinear\", \"nearest\"),\n",
        "      ),\n",
        "      RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[128, 128, 128], random_size=False),\n",
        "      RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
        "      RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
        "      RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
        "      NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "      RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
        "      RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
        "  ])\n",
        "\n",
        "  val_transform = Compose([\n",
        "      LoadImaged(keys=[\"image\", \"label\"]),\n",
        "      EnsureChannelFirstd(keys=[\"image\",\"label\"]),\n",
        "      EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "      Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
        "      Spacingd(\n",
        "          keys=[\"image\", \"label\"],\n",
        "          pixdim=(1.0, 1.0, 1.0),\n",
        "          mode=(\"bilinear\", \"nearest\"),\n",
        "      ),\n",
        "      NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
        "  ])\n",
        "\n",
        "  train_ds = monai.data.Dataset(data=train_files, transform=train_transform)\n",
        "  train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=2)\n",
        "\n",
        "  val_ds = monai.data.Dataset(data=val_files, transform=val_transform)\n",
        "  val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "  return train_loader, val_loader, val_ds"
      ],
      "metadata": {
        "id": "6EB_QmNK25uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_and_train(train_loader, val_loader, max_epochs, val_ds):\n",
        "  val_interval = 1\n",
        "  VAL_AMP = True\n",
        "\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  model = SegResNet(\n",
        "    blocks_down=[1, 2, 2, 4],\n",
        "    blocks_up=[1, 1, 1],\n",
        "    init_filters=16,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    dropout_prob=0.2,\n",
        "  ).to(device)\n",
        "\n",
        "  loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), 1e-3, weight_decay=1e-6)\n",
        "  lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
        "\n",
        "  dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "  dice_metric_train = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "\n",
        "  post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
        "\n",
        "  scaler = torch.cuda.amp.GradScaler()\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "\n",
        "  epoch_loss_values = []\n",
        "  metric_values = []\n",
        "  metric_values_train = []\n",
        "  total_start = time.time()\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    epoch_start = time.time()\n",
        "    print(\"-\" * 10)\n",
        "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    for batch_data in train_loader:\n",
        "        step_start = time.time()\n",
        "        step += 1\n",
        "        inputs, labels = ( batch_data[\"image\"].to(device), batch_data[\"label\"].to(device) )\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "\n",
        "        outputs = [post_trans(i) for i in decollate_batch(outputs)]\n",
        "\n",
        "        dice_metric_train(y_pred=outputs, y=labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    metric_train = dice_metric_train.aggregate().item()\n",
        "    metric_values_train.append(metric_train)\n",
        "    dice_metric_train.reset()\n",
        "\n",
        "    lr_scheduler.step()\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    print(f\"Loss: {epoch_loss:.4f} \\nTrain dice: {metric_train:.4f}\")\n",
        "\n",
        "    if (epoch + 1) % val_interval == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_data in val_loader:\n",
        "                val_inputs, val_labels = (val_data[\"image\"].to(device),val_data[\"label\"].to(device))\n",
        "\n",
        "                val_outputs = model(val_inputs)\n",
        "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
        "\n",
        "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
        "\n",
        "            metric = dice_metric.aggregate().item()\n",
        "            metric_values.append(metric)\n",
        "            dice_metric.reset()\n",
        "\n",
        "            print(f\"Test dice: {metric:.4f}\")\n",
        "\n",
        "    print(f\"Time: {(time.time() - epoch_start):.4f}\")\n",
        "  total_time = time.time() - total_start\n",
        "  print(f\"Train completed, total time: {total_time}.\")\n",
        "  print()\n",
        "  print(f\"Train metric = {metric_values_train[-1]}, Test metric = {metric_values[-1]}\")\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    i = 0\n",
        "\n",
        "    val_input = val_ds[i][\"image\"].unsqueeze(0).to(device)\n",
        "\n",
        "    val_output = model(val_input)\n",
        "    val_output = post_trans(val_output[0])\n",
        "\n",
        "    plt.figure(\"fig\")\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12,6))\n",
        "\n",
        "    ax1.set_title(\"Image slice\")\n",
        "    ax1.imshow(val_ds[i][\"image\"][0, :, :, 64].detach().cpu(), cmap=\"gray\")\n",
        "\n",
        "    ax2.set_title(\"Label slice\")\n",
        "    ax2.imshow(val_ds[i][\"label\"][0, :, :, 64].detach().cpu())\n",
        "\n",
        "    ax3.set_title(\"Output slice\")\n",
        "    ax3.imshow(val_output[0, :, :, 64].detach().cpu())\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, subplot_kw={\"projection\": \"3d\"},figsize = (12,6))\n",
        "\n",
        "    z, x, y = val_ds[i][\"label\"][0].astype(np.uint8).nonzero()\n",
        "    ax[0].scatter(x, y, z)\n",
        "    ax[0].set_xlim([0,128])\n",
        "    ax[0].set_ylim([0,128])\n",
        "    ax[0].set_title(\"Label\")\n",
        "\n",
        "    z, x, y = val_output[0].astype(np.uint8).nonzero()\n",
        "    ax[1].scatter(x, y, z)\n",
        "    ax[1].set_xlim([0,128])\n",
        "    ax[1].set_ylim([0,128])\n",
        "    ax[1].set_title(\"Output\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  return metric_values_train[-1], metric_values[-1], epoch_loss_values[-1]"
      ],
      "metadata": {
        "id": "moXDkv0d3biq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_dim(n_train_vec, metric_train, metric_test, loss, r_min, r_max, noise, max_epochs):\n",
        "  fig, ax = plt.subplots(1, 2, figsize = (12,6))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(n_train_vec, metric_train, label = 'Train metric')\n",
        "  plt.plot(n_train_vec, metric_test, label = 'Test metric')\n",
        "  plt.xscale('log', base = 2)\n",
        "  plt.xlabel(\"Train dimention\")\n",
        "  plt.title(f\"noise = {noise}, epochs = {max_epochs}, r = ({r_min}, {r_max})\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(n_train_vec, loss, label = 'Train loss')\n",
        "  plt.xscale('log', base = 2)\n",
        "  plt.xlabel(\"Train dimention\")\n",
        "  plt.title(f\"noise = {noise}, epochs = {max_epochs}, r = ({r_min}, {r_max})\")\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "VQgE1cFd4iqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_noise(noise_vec, metric_train, metric_test, loss, r_min, r_max, max_epochs):\n",
        "  fig, ax = plt.subplots(1, 2, figsize = (12,6))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(noise_vec, metric_train, label = 'Train metric')\n",
        "  plt.plot(noise_vec, metric_test, label = 'Test metric')\n",
        "  plt.xlabel(\"Noise\")\n",
        "  plt.title(f\"epochs = {max_epochs}, r = ({r_min}, {r_max})\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(noise_vec, loss, label = 'Train loss')\n",
        "  plt.xlabel(\"Noise\")\n",
        "  plt.title(f\"epochs = {max_epochs}, r = ({r_min}, {r_max})\")\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "lwi_Gcx6xGJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_image(directory_path):\n",
        "  file_list = os.listdir(directory_path)\n",
        "  for file in file_list:\n",
        "      file_path = os.path.join(directory_path, file)\n",
        "      if os.path.isfile(file_path):\n",
        "          os.remove(file_path)\n",
        "      elif os.path.isdir(file_path):\n",
        "          os.rmdir(file_path)\n",
        "  print()"
      ],
      "metadata": {
        "id": "eg5tVBl24vDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute training\n",
        "Specify:\n",
        "- minimum radius (\"r_min\": int)\n",
        "- maximum radius (\"r_max\": int)\n",
        "- noise to be added (\"noise\": float)\n",
        "- training epochs (\"max_epochs\": int)"
      ],
      "metadata": {
        "id": "u7sSU8mnGZVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimention train samples"
      ],
      "metadata": {
        "id": "dfKHZqqHxIud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_train_vec  = [8, 16, 32, 64]\n",
        "n_test = 20\n",
        "\n",
        "noise = 1\n",
        "r_min = 5\n",
        "r_max = 10\n",
        "\n",
        "max_epochs = 25\n",
        "\n",
        "metric_values_train_vec = []\n",
        "metric_values_vec = []\n",
        "epoch_loss_values_vec = []\n",
        "\n",
        "\n",
        "for n_train in n_train_vec:\n",
        "  print()\n",
        "  print(f\"Number of training images = {n_train}\")\n",
        "  print(f\"Number of testing images = {n_test}\")\n",
        "  print()\n",
        "\n",
        "  train_files, val_files = create_image_3d(n_train, n_test, noise, r_min, r_max)\n",
        "  train_loader, val_loader, val_ds = transform(train_files, val_files)\n",
        "\n",
        "  metric_value_train, metric_value, epoch_loss_value = model_and_train(train_loader, val_loader, max_epochs, val_ds)\n",
        "\n",
        "  metric_values_train_vec.append(metric_value_train)\n",
        "  metric_values_vec.append(metric_value)\n",
        "  epoch_loss_values_vec.append(epoch_loss_value)\n",
        "\n",
        "  delete_image(directory_path)\n",
        "\n",
        "plot_train_dim(n_train_vec, metric_values_train_vec, metric_values_vec, epoch_loss_values_vec, r_min, r_max, noise, max_epochs)"
      ],
      "metadata": {
        "id": "YKLf6jLuGcKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise"
      ],
      "metadata": {
        "id": "uTAb4JhyTSxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = 30\n",
        "n_test = 20\n",
        "\n",
        "noise_vec = [0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5]\n",
        "r_min = 5\n",
        "r_max = 10\n",
        "\n",
        "max_epochs = 25\n",
        "\n",
        "metric_values_train_vec = []\n",
        "metric_values_vec = []\n",
        "epoch_loss_values_vec = []\n",
        "\n",
        "\n",
        "for noise in noise_vec:\n",
        "  print()\n",
        "  print(f\"Noise = {noise}\")\n",
        "  print()\n",
        "\n",
        "  train_files, val_files = create_image_2d(n_train, n_test, noise, r_min, r_max)\n",
        "  train_loader, val_loader, val_ds = transform(train_files, val_files)\n",
        "\n",
        "  metric_value_train, metric_value, epoch_loss_value = model_and_train(train_loader, val_loader, max_epochs, val_ds)\n",
        "\n",
        "  metric_values_train_vec.append(metric_value_train)\n",
        "  metric_values_vec.append(metric_value)\n",
        "  epoch_loss_values_vec.append(epoch_loss_value)\n",
        "\n",
        "  delete_image(directory_path)\n",
        "\n",
        "plot_noise(noise_vec, metric_values_train_vec, metric_values_vec, epoch_loss_values_vec, r_min, r_max, max_epochs)"
      ],
      "metadata": {
        "id": "jA43B-3QSR5j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}